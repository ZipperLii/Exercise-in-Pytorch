{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "<built-in method numel of Tensor object at 0x000001C257C1A5C0>\n",
      "reshaped X: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 常用的torch创建方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "<built-in method numel of Tensor object at 0x000001C257CAD120>\n"
     ]
    }
   ],
   "source": [
    "# 创建元素从0到12的张量\n",
    "x = torch.arange(end=12)\n",
    "print(x.shape)\n",
    "print(x.numel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped X: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# reshape修改维度\n",
    "X = x.reshape(3,4)\n",
    "print(f\"reshaped X: {X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 创建全为0或1的张量\n",
    "ones = torch.ones(2,3,4)\n",
    "zeros = torch.zeros(2,3,4)\n",
    "print(ones)\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 广播机制\n",
    "形状不同的张量也可以相加，导致结果错误\\\n",
    "a = (0,1,2), b = (0,1,2)$^\\top$\\\n",
    "a+b=$\\begin{bmatrix}\n",
    "  0&  1& 2\\\\\n",
    "  0&  1& 2\\\\\n",
    "  0&  1& 2\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "  0& 0 & 0\\\\\n",
    "  1& 1 & 1\\\\\n",
    "  2&  2&2\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "  0& 1 & 2\\\\\n",
    "  1& 2 & 3\\\\\n",
    "  2& 3 & 4\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [1, 2, 3],\n",
       "        [2, 3, 4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape(1,3)\n",
    "b = torch.arange(3).reshape(3,1)\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切片机制\n",
    "x_ = torch.ones((3, 4))\n",
    "x_[1:3,::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. tensor转numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(3)\n",
    "b = a.numpy()\n",
    "print(type(a), type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9, 18, 27])\n",
      "tensor([18, 18, 18])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(27).reshape((3,3,3))\n",
    "a[0,:,:] = 1\n",
    "a[1,:,:] = 2\n",
    "a[2,:,:] = 3\n",
    "# 固定后两个维度求和\n",
    "print(a.sum(axis=[1,2]))\n",
    "\n",
    "# 固定前两个梯度求和\n",
    "print(a.sum(axis=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. backward()求导数\n",
    "1. 对于y=f(x), 创建x时需要x.requires_grad_(True) 或 在定义中给定参数 requires_grad=True\n",
    "2. 将x输入到y之后，使用y.backward()对x求导\n",
    "3. x.grad.zero()将x的梯度置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  4.,  8., 12.])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def gradient(x):\n",
    "    y = 2 * torch.dot(x, x)\n",
    "    y.backward()\n",
    "    return x.grad\n",
    "\n",
    "x = torch.arange(4.0).requires_grad_(True)\n",
    "a = gradient(x)\n",
    "print(a)\n",
    "# 求导后将导数置0\n",
    "x.grad.zero_()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 张量复制（分配新内存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape((5,4))\n",
    "B = A.clone()\n",
    "A * 2 == A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量求和操作\n",
    "1. keepdims=True参数保持某一维度不求和\n",
    "2. 利用广播机制求和,A / A.sum\n",
    "3. 某个轴计算A的元素的累计总和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[40., 45., 50., 55.]])\n",
      "tensor([[0.0000, 0.0222, 0.0400, 0.0545],\n",
      "        [0.1000, 0.1111, 0.1200, 0.1273],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.3000, 0.2889, 0.2800, 0.2727],\n",
      "        [0.4000, 0.3778, 0.3600, 0.3455]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape((5,4))\n",
    "A_sum = A.sum(axis=0, keepdims=True) # axis = 0 使得A的行保持不求和,对列求和\n",
    "A_sum1 = A.cumsum(axis=0)\n",
    "print(A_sum)\n",
    "print(A / A_sum)\n",
    "print(A_sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
